{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><div align=\"center\">Deep Learning From Scratch</div></h1>\n",
    "<h2><div align=\"center\">Modular Network</div></h2>\n",
    "<div align=\"center\">Bruno Gon√ßalves</div>\n",
    "<div align=\"center\"><a href=\"http://www.data4sci.com/\">www.data4sci.com</a></div>\n",
    "<div align=\"center\">@bgoncalves</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import watermark\n",
    "\n",
    "%load_ext watermark\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr 19 2019 \n",
      "\n",
      "CPython 3.7.3\n",
      "IPython 7.4.0\n",
      "\n",
      "numpy 1.16.2\n",
      "matplotlib 3.0.3\n",
      "pandas 0.24.2\n",
      "seaborn 0.9.0\n",
      "\n",
      "compiler   : Clang 4.0.1 (tags/RELEASE_401/final)\n",
      "system     : Darwin\n",
      "release    : 18.2.0\n",
      "machine    : x86_64\n",
      "processor  : i386\n",
      "CPU cores  : 8\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%watermark -n -v -m -p numpy,matplotlib,pandas,seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('input/X_train.npy')\n",
    "X_test = np.load('input/X_test.npy')\n",
    "y_train = np.load('input/y_train.npy')\n",
    "y_test = np.load('input/y_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size = X_train.shape[1]\n",
    "\n",
    "X_train /= 255.\n",
    "X_test /= 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the initializatino function as we'll have to call it more than once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(L_in, L_out):\n",
    "    epsilon = 0.12\n",
    "\n",
    "    return 2*np.random.rand(L_out, L_in+1)*epsilon - epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the layer sizes we'll be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_size = 50\n",
    "num_labels = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the weights. In this case we use a array of weight matrices so that we can easily add/remove layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Thetas = []\n",
    "Thetas.append(init_weights(input_layer_size, hidden_layer_size))\n",
    "Thetas.append(init_weights(hidden_layer_size, num_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding to define the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(K, pos):\n",
    "    y0 = np.zeros(K)\n",
    "    y0[pos] = 1\n",
    "\n",
    "    return y0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation function base class. Here we must provide an interface to both the activation function and its derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(object):\n",
    "    def f(z):\n",
    "        pass\n",
    "\n",
    "    def df(z):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The various activation functions simply extend the base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Activation):\n",
    "    def f(z):\n",
    "        return z\n",
    "\n",
    "    def df(z):\n",
    "        return np.ones(z.shape)\n",
    "\n",
    "class ReLu(Activation):\n",
    "    def f(z):\n",
    "        return np.where(z > 0, z, 0)\n",
    "\n",
    "    def df(z):\n",
    "        return np.where(z > 0, 1, 0)\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    def f(z):\n",
    "        return 1./(1+np.exp(-z))\n",
    "    \n",
    "    def df(z):\n",
    "        h = Sigmoid.f(z)\n",
    "        return h*(1-h)\n",
    "\n",
    "class TanH(Activation):\n",
    "    def f(z):\n",
    "        return np.tanh(z)\n",
    "\n",
    "    def df(z):\n",
    "        return 1-np.power(np.tanh(z), 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation and Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward and predict functions are also generalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(Theta, X, active):\n",
    "    N = X.shape[0]\n",
    "\n",
    "    # Add the bias column\n",
    "    X_ = np.concatenate((np.ones((N, 1)), X), 1)\n",
    "\n",
    "    # Multiply by the weights\n",
    "    z = np.dot(X_, Theta.T)\n",
    "\n",
    "    # Apply the activation function\n",
    "    a = active.f(z)\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predict function now takes the entire model as input and it must loop over the various layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X):\n",
    "    h = X.copy()\n",
    "\n",
    "    for i in range(0, len(model), 2):\n",
    "        theta = model[i]\n",
    "        activation = model[i+1]\n",
    "\n",
    "        h = forward(theta, h, activation)\n",
    "\n",
    "    return np.argmax(h, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy function is just the same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_, y):\n",
    "    return np.mean((y_ == y.flatten()))*100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(model, X, y):\n",
    "    M = X.shape[0]\n",
    "\n",
    "    Thetas=[0]\n",
    "    Thetas.extend(model[0::2])\n",
    "    activations = [0]\n",
    "    activations.extend(model[1::2])\n",
    "\n",
    "    layers = len(Thetas)\n",
    "\n",
    "    K = Thetas[-1].shape[0]\n",
    "    J = 0\n",
    "\n",
    "    Deltas = [0]\n",
    "\n",
    "    for i in range(1, layers):\n",
    "        Deltas.append(np.zeros(Thetas[i].shape))\n",
    "\n",
    "    deltas = [0]*(layers+1)\n",
    "\n",
    "    for i in range(M):\n",
    "        As = [0]\n",
    "        Zs = [0, 0]\n",
    "        Hs = [0, X[i]]\n",
    "\n",
    "        # Forward propagation, saving intermediate results\n",
    "        As.append(np.concatenate(([1], Hs[1])))  # Input layer\n",
    "\n",
    "        for l in range(2, layers+1):\n",
    "            Zs.append(np.dot(Thetas[l-1], As[l-1]))\n",
    "            Hs.append(activations[l-1].f(Zs[l]))\n",
    "            As.append(np.concatenate(([1], Hs[l])))\n",
    "\n",
    "        y0 = one_hot(K, y[i])\n",
    "\n",
    "        # Cross entropy\n",
    "        J -= np.dot(y0.T, np.log(Hs[-1]))+np.dot((1-y0).T, np.log(1-Hs[-1]))\n",
    "\n",
    "        deltas[layers] = Hs[layers]-y0\n",
    "\n",
    "        # Calculate the weight deltas\n",
    "        for l in range(layers-1, 1, -1):\n",
    "            deltas[l] = np.dot(Thetas[l].T, deltas[l+1])[1:]*activations[l].df(Zs[l])\n",
    "\n",
    "        Deltas[2] += np.outer(deltas[3], As[2])\n",
    "        Deltas[1] += np.outer(deltas[2], As[1])\n",
    "\n",
    "    J /= M\n",
    "\n",
    "    grads = []\n",
    "\n",
    "    grads.append(Deltas[1]/M)\n",
    "    grads.append(Deltas[2]/M)\n",
    "\n",
    "    return [J, grads]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = []\n",
    "\n",
    "model.append(Thetas[0])\n",
    "model.append(Sigmoid)\n",
    "model.append(Thetas[1])\n",
    "model.append(Sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training procedure\n",
    "The same basic idea as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 1.334084760581442 1.4427326285317454 83.12 80.10000000000001\n",
      "200 0.8558669802854713 1.001240130539889 89.64 86.7\n",
      "300 0.6713424883319848 0.8393312154768764 91.72 88.0\n",
      "400 0.5741679051606795 0.7574224004163427 92.64 88.3\n",
      "500 0.5104217151711248 0.7057397966332198 93.38 89.1\n",
      "600 0.46311692208098504 0.6687704640775677 93.96 89.5\n",
      "700 0.42536200174129163 0.6403302703235646 94.5 89.4\n",
      "800 0.3937998980163512 0.6174326499662504 94.94 90.2\n",
      "900 0.36659814539585817 0.5984187382247174 95.44 90.5\n",
      "1000 0.3426684474089093 0.582276468248388 95.76 90.8\n",
      "1100 0.32131668712856193 0.5683479284243107 96.06 91.0\n",
      "1200 0.302069962538266 0.5561793061910165 96.3 91.2\n",
      "1300 0.2845828413234672 0.5454397916911339 96.64 91.2\n",
      "1400 0.26859073169509906 0.5358817130670512 96.89999999999999 91.3\n",
      "1500 0.25388689763896394 0.527317707942177 97.14 91.4\n",
      "1600 0.2403077415000062 0.5196042591504781 97.28 91.8\n",
      "1700 0.22772236428571752 0.5126292515252665 97.46000000000001 91.9\n",
      "1800 0.21602506759129478 0.5063017242340844 97.68 92.2\n",
      "1900 0.20512898933818802 0.5005435088915905 97.88 92.5\n",
      "2000 0.19496040041004026 0.4952842618850056 98.02 92.4\n",
      "2100 0.1854545503258079 0.49046019837435 98.18 92.4\n",
      "2200 0.17655343244846808 0.48601466947194355 98.28 92.5\n",
      "2300 0.1682048506894025 0.4818987645538667 98.36 92.5\n",
      "2400 0.16036196201618355 0.4780712610691598 98.46000000000001 92.4\n",
      "2500 0.15298281874047753 0.4744979488586314 98.61999999999999 92.4\n",
      "2600 0.14602981413610858 0.47115057513234965 98.76 92.4\n",
      "2700 0.13946913331979918 0.46800558040983037 98.86 92.30000000000001\n",
      "2800 0.13327041857830976 0.46504254708972653 98.98 92.30000000000001\n",
      "2900 0.12740698090993818 0.4622421538482467 99.03999999999999 92.30000000000001\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "tol = 1e-5\n",
    "J_old = 1/tol\n",
    "diff = 1\n",
    "\n",
    "acc_train = []\n",
    "acc_test = []\n",
    "J_val = []\n",
    "steps = []\n",
    "\n",
    "while diff > tol:\n",
    "    J_train, grads = backprop(model, X_train, y_train)\n",
    "\n",
    "    diff = abs(J_old-J_train)\n",
    "    J_old = J_train\n",
    "    J_val.append(J_train)\n",
    "\n",
    "    step += 1\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        pred_train = predict(model, X_train)\n",
    "        pred_test = predict(model, X_test)\n",
    "\n",
    "        J_test, grads = backprop(model, X_test, y_test)\n",
    "\n",
    "        acc_train.append(accuracy(pred_train, y_train))\n",
    "        acc_test.append(accuracy(pred_test, y_test))\n",
    "        steps.append(step)\n",
    "        \n",
    "        print(step, J_train, J_test, acc_train[-1], acc_test[-1])\n",
    "\n",
    "    for i in range(len(Thetas)):\n",
    "        Thetas[i] -= .5*grads[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1, len(J_val)+1), J_val)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Cost function\")\n",
    "plt.gcf().set_size_inches(11, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(steps, acc_train, label='Training dataset')\n",
    "plt.plot(steps, acc_test, label='Testing dataset')\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "plt.gcf().set_size_inches(11, 8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
