{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><div align=\"center\">Deep Learning From Scratch</div></h1>\n",
    "<h2><div align=\"center\">Simple Network</div></h2>\n",
    "<div align=\"center\">Bruno Gon√ßalves</div>\n",
    "<div align=\"center\"><a href=\"http://www.data4sci.com/\">www.data4sci.com</a></div>\n",
    "<div align=\"center\">@bgoncalves</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import watermark\n",
    "\n",
    "%load_ext watermark\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr 19 2019 \n",
      "\n",
      "CPython 3.7.3\n",
      "IPython 7.4.0\n",
      "\n",
      "numpy 1.16.2\n",
      "matplotlib 3.0.3\n",
      "pandas 0.24.2\n",
      "seaborn 0.9.0\n",
      "\n",
      "compiler   : Clang 4.0.1 (tags/RELEASE_401/final)\n",
      "system     : Darwin\n",
      "release    : 18.2.0\n",
      "machine    : x86_64\n",
      "processor  : i386\n",
      "CPU cores  : 8\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%watermark -n -v -m -p numpy,matplotlib,pandas,seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('input/X_train.npy')\n",
    "X_test = np.load('input/X_test.npy')\n",
    "y_train = np.load('input/y_train.npy')\n",
    "y_test = np.load('input/y_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size = X_train.shape[1]\n",
    "\n",
    "X_train /= 255.\n",
    "X_test /= 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the initializatino function as we'll have to call it more than once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(L_in, L_out):\n",
    "    epsilon = 0.12\n",
    "\n",
    "    return 2*np.random.rand(L_out, L_in+1)*epsilon - epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the layer sizes we'll be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_size = 50\n",
    "num_labels = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta1 = init_weights(input_layer_size, hidden_layer_size)\n",
    "Theta2 = init_weights(hidden_layer_size, num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding to define the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(K, pos):\n",
    "    y0 = np.zeros(K)\n",
    "    y0[pos] = 1\n",
    "\n",
    "    return y0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation function, just as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1./(1+np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoidGradient(z):\n",
    "    h = sigmoid(z)\n",
    "    return h*(1-h)\n",
    "\n",
    "def accuracy(y_, y):\n",
    "    return np.mean((y_ == y.flatten()))*100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation and Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same functions defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(Theta, X, active):\n",
    "    N = X.shape[0]\n",
    "\n",
    "    # Add the bias column\n",
    "    X_ = np.concatenate((np.ones((N, 1)), X), 1)\n",
    "\n",
    "    # Multiply by the weights\n",
    "    z = np.dot(X_, Theta.T)\n",
    "\n",
    "    # Apply the activation function\n",
    "    a = active(z)\n",
    "\n",
    "    return a\n",
    "\n",
    "def predict(Theta1, Theta2, X):\n",
    "    h1 = forward(Theta1, X, sigmoid)\n",
    "    h2 = forward(Theta2, h1, sigmoid)\n",
    "\n",
    "    return np.argmax(h2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(Theta1, Theta2, X, y):\n",
    "    N = X.shape[0]\n",
    "    K = Theta2.shape[0]\n",
    "\n",
    "    J = 0\n",
    "\n",
    "    Delta2 = np.zeros(Theta2.shape)\n",
    "    Delta1 = np.zeros(Theta1.shape)\n",
    "\n",
    "    for i in range(N):\n",
    "        # Forward propagation, saving intermediate results\n",
    "        a1 = np.concatenate(([1], X[i]))  # Input layer\n",
    "\n",
    "        z2 = np.dot(Theta1, a1)\n",
    "        a2 = np.concatenate(([1], sigmoid(z2)))  # Hidden Layer\n",
    "\n",
    "        z3 = np.dot(Theta2, a2)\n",
    "        a3 = sigmoid(z3)  # Output layer\n",
    "\n",
    "        y0 = one_hot(K, y[i])\n",
    "\n",
    "        # Cross entropy\n",
    "        J -= np.dot(y0.T, np.log(a3))+np.dot((1-y0).T, np.log(1-a3))\n",
    "\n",
    "        # Calculate the weight deltas\n",
    "        delta_3 = a3-y0\n",
    "        delta_2 = np.dot(Theta2.T, delta_3)[1:]*sigmoidGradient(z2)\n",
    "\n",
    "        Delta2 += np.outer(delta_3, a2)\n",
    "        Delta1 += np.outer(delta_2, a1)\n",
    "\n",
    "    J /= N\n",
    "\n",
    "    Theta1_grad = Delta1/N\n",
    "    Theta2_grad = Delta2/N\n",
    "\n",
    "    return [J, Theta1_grad, Theta2_grad]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training procedure\n",
    "The basic idea is the same as for the linear and logistic regression cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 3.1291753860666778 3.136960891294815 43.580000000000005 43.0\n",
      "20 2.9167902906936987 2.9446602205385557 59.160000000000004 55.1\n",
      "30 2.603769168076834 2.6579288877993905 62.96000000000001 59.699999999999996\n",
      "40 2.2804339384826813 2.3564282797253466 66.97999999999999 63.7\n",
      "50 2.020640681525116 2.1098938447015403 71.41999999999999 68.30000000000001\n",
      "60 1.8217778527123045 1.9191424477811603 75.3 71.89999999999999\n",
      "70 1.6635058765800954 1.766777938511813 78.24 75.0\n",
      "80 1.5326041982455285 1.6408468843796575 80.86 76.9\n",
      "90 1.4222146978617052 1.534979159378096 82.72 78.10000000000001\n",
      "100 1.3283330821233696 1.4453966968730128 83.91999999999999 80.80000000000001\n",
      "110 1.2479904329251943 1.369254959226355 85.1 81.69999999999999\n",
      "120 1.178707428440679 1.3041265102022936 85.88 83.0\n",
      "130 1.118424693106811 1.247947341912282 86.9 83.8\n",
      "140 1.0654957115754506 1.199036214958088 87.46000000000001 84.8\n",
      "150 1.0186392323760602 1.1560717547952608 88.12 84.89999999999999\n",
      "160 0.9768659071718415 1.118033253058756 88.58 85.2\n",
      "170 0.9394056133151405 1.0841322121734045 89.05999999999999 85.6\n",
      "180 0.9056481046266502 1.053751545724964 89.48 86.1\n",
      "190 0.8750990997302545 1.0263983121625406 89.86 86.1\n",
      "200 0.847349502717181 1.00166976267639 89.96 86.3\n",
      "210 0.8220544467127464 0.9792301070635664 90.12 86.5\n",
      "220 0.7989191639215982 0.9587949836555328 90.36 86.6\n",
      "230 0.7776893554056528 0.9401210445107877 90.56 86.9\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "tol = 1e-4\n",
    "J_old = 1/tol\n",
    "diff = 1\n",
    "\n",
    "acc_train = []\n",
    "acc_test = []\n",
    "J_val = []\n",
    "steps = []\n",
    "\n",
    "while diff > tol:\n",
    "    J_train, Theta1_grad, Theta2_grad = backprop(Theta1, Theta2, X_train, y_train)\n",
    "\n",
    "    diff = abs(J_old-J_train)\n",
    "    J_old = J_train\n",
    "    J_val.append(J_train)\n",
    "    \n",
    "    step += 1\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        pred_train = predict(Theta1, Theta2, X_train)\n",
    "        pred_test = predict(Theta1, Theta2, X_test)\n",
    "\n",
    "        J_test, T1_grad, T2_grad = backprop(Theta1, Theta2, X_test, y_test)\n",
    "        \n",
    "        acc_train.append(accuracy(pred_train, y_train))\n",
    "        acc_test.append(accuracy(pred_test, y_test))\n",
    "        steps.append(step)\n",
    "        \n",
    "        print(step, J_train, J_test, acc_train[-1], acc_test[-1])\n",
    "\n",
    "    Theta1 -= .5*Theta1_grad\n",
    "    Theta2 -= .5*Theta2_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1, len(J_val)+1), J_val)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Cost function\")\n",
    "plt.gcf().set_size_inches(11, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(steps, acc_train, label='Training dataset')\n",
    "plt.plot(steps, acc_test, label='Testing dataset')\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "plt.gcf().set_size_inches(11, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict(Theta1, Theta2, X_test)\n",
    "accuracy(pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Theta1.npy', Theta1)\n",
    "np.save('Theta2.npy', Theta2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
