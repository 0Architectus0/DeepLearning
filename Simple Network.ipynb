{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; overflow: hidden;\">\n",
    "    <div style=\"width: 150px; float: left;\"> <img src=\"data/D4Sci_logo_ball.png\" alt=\"Data For Science, Inc\" align=\"left\" border=\"0\"> </div>\n",
    "    <div style=\"float: left; margin-left: 10px;\"> <h1>Deep Learning From Scratch</h1>\n",
    "<h2>Simple Network</h2>\n",
    "        <p>Bruno Gon√ßalves<br/>\n",
    "        <a href=\"http://www.data4sci.com/\">www.data4sci.com</a><br/>\n",
    "            @bgoncalves, @data4sci</p></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import watermark\n",
    "\n",
    "%load_ext watermark\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas     0.24.2\n",
      "numpy      1.16.2\n",
      "watermark  1.8.1\n",
      "seaborn    0.9.0\n",
      "matplotlib 3.1.0\n",
      "Mon Sep 23 2019 2019-09-23T22:04:28-04:00\n",
      "\n",
      "CPython 3.7.3\n",
      "IPython 6.2.1\n",
      "\n",
      "compiler   : Clang 4.0.1 (tags/RELEASE_401/final)\n",
      "system     : Darwin\n",
      "release    : 18.7.0\n",
      "machine    : x86_64\n",
      "processor  : i386\n",
      "CPU cores  : 8\n",
      "interpreter: 64bit\n",
      "Git hash   : 257ca1850405e2c214a57bc0c4ccdc6e02a54dcc\n"
     ]
    }
   ],
   "source": [
    "%watermark -i -n -v -m -g -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('input/X_train.npy')\n",
    "X_test = np.load('input/X_test.npy')\n",
    "y_train = np.load('input/y_train.npy')\n",
    "y_test = np.load('input/y_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size = X_train.shape[1]\n",
    "\n",
    "X_train /= 255.\n",
    "X_test /= 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the initializatino function as we'll have to call it more than once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(L_in, L_out):\n",
    "    epsilon = 0.12\n",
    "\n",
    "    return 2*np.random.rand(L_out, L_in+1)*epsilon - epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the layer sizes we'll be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_size = 50\n",
    "num_labels = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta1 = init_weights(input_layer_size, hidden_layer_size)\n",
    "Theta2 = init_weights(hidden_layer_size, num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding to define the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(K, pos):\n",
    "    y0 = np.zeros(K)\n",
    "    y0[pos] = 1\n",
    "\n",
    "    return y0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation function, just as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1./(1+np.exp(-z))\n",
    "\n",
    "def sigmoidGradient(z):\n",
    "    h = sigmoid(z)\n",
    "    return h*(1-h)\n",
    "\n",
    "def accuracy(y_, y):\n",
    "    return np.mean((y_ == y.flatten()))*100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation and Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same functions defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(Theta, X, active):\n",
    "    N = X.shape[0]\n",
    "\n",
    "    # Add the bias column\n",
    "    X_ = np.concatenate((np.ones((N, 1)), X), 1)\n",
    "\n",
    "    # Multiply by the weights\n",
    "    z = np.dot(X_, Theta.T)\n",
    "\n",
    "    # Apply the activation function\n",
    "    a = active(z)\n",
    "\n",
    "    return a\n",
    "\n",
    "def predict(Theta1, Theta2, X):\n",
    "    h1 = forward(Theta1, X, sigmoid)\n",
    "    h2 = forward(Theta2, h1, sigmoid)\n",
    "\n",
    "    return np.argmax(h2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(Theta1, Theta2, X, y):\n",
    "    N = X.shape[0]\n",
    "    K = Theta2.shape[0]\n",
    "\n",
    "    J = 0\n",
    "\n",
    "    Delta2 = np.zeros(Theta2.shape)\n",
    "    Delta1 = np.zeros(Theta1.shape)\n",
    "\n",
    "    for i in range(N):\n",
    "        # Forward propagation, saving intermediate results\n",
    "        a1 = np.concatenate(([1], X[i]))  # Input layer\n",
    "\n",
    "        z2 = np.dot(Theta1, a1)\n",
    "        a2 = np.concatenate(([1], sigmoid(z2)))  # Hidden Layer\n",
    "\n",
    "        z3 = np.dot(Theta2, a2)\n",
    "        a3 = sigmoid(z3)  # Output layer\n",
    "\n",
    "        y0 = one_hot(K, y[i])\n",
    "\n",
    "        # Cross entropy\n",
    "        J -= np.dot(y0.T, np.log(a3))+np.dot((1-y0).T, np.log(1-a3))\n",
    "\n",
    "        # Calculate the weight deltas\n",
    "        delta_3 = a3-y0\n",
    "        delta_2 = np.dot(Theta2.T, delta_3)[1:]*sigmoidGradient(z2)\n",
    "\n",
    "        Delta2 += np.outer(delta_3, a2)\n",
    "        Delta1 += np.outer(delta_2, a1)\n",
    "\n",
    "    J /= N\n",
    "\n",
    "    Theta1_grad = Delta1/N\n",
    "    Theta2_grad = Delta2/N\n",
    "\n",
    "    return [J, Theta1_grad, Theta2_grad]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training procedure\n",
    "The basic idea is the same as for the linear and logistic regression cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 3.1021957486777243 3.114371087115743 44.82 40.8\n",
      "20 2.8631672462529414 2.896384721063332 59.36 54.6\n",
      "30 2.5300753528786184 2.5875899670356874 64.88000000000001 60.6\n",
      "40 2.2060963355068623 2.281535133202134 69.84 65.5\n",
      "50 1.9530141048759992 2.0387148022816386 74.33999999999999 70.1\n",
      "60 1.7607729577654896 1.8528780908989575 76.98 72.2\n",
      "70 1.6092972311542026 1.7063544371820634 79.66 74.1\n",
      "80 1.4859581517290104 1.5874686146948322 81.47999999999999 76.8\n",
      "90 1.3833719686757076 1.4891837372361867 83.24000000000001 78.60000000000001\n",
      "100 1.2966828982642402 1.4067574760538806 84.36 79.7\n",
      "110 1.2223987210861267 1.3367090427868664 85.42 81.2\n",
      "120 1.1579472257335441 1.276432978164113 86.18 82.39999999999999\n",
      "130 1.101435866194006 1.2239919515792623 86.8 83.3\n",
      "140 1.0514678206698211 1.177951674354346 87.32 84.2\n",
      "150 1.0069949688523325 1.1372413114872202 87.92 84.6\n",
      "160 0.9672083801675815 1.1010435875038818 88.48 85.3\n",
      "170 0.931462714697286 1.0687151188104085 88.92 85.6\n",
      "180 0.8992269964150311 1.039732625861601 89.44 86.2\n",
      "190 0.8700537180408324 1.0136586121783326 89.7 86.5\n",
      "200 0.8435596681996527 0.9901202409856449 89.86 86.9\n",
      "210 0.8194137714892993 0.9687963689844573 90.03999999999999 87.1\n",
      "220 0.797328897053348 0.9494091997791942 90.28 87.4\n",
      "230 0.777055823726768 0.9317183523667351 90.42 87.3\n",
      "240 0.7583783719993665 0.9155161289424227 90.64 87.8\n",
      "250 0.7411092142287737 0.9006234025903002 90.94 87.7\n",
      "260 0.725086150805799 0.8868859007272953 91.06 87.9\n",
      "270 0.7101687735155994 0.8741708269560937 91.10000000000001 87.9\n",
      "280 0.6962354898858111 0.8623638226617607 91.22 87.9\n",
      "290 0.6831808949833579 0.8513662765515623 91.32000000000001 87.9\n",
      "300 0.6709134740095456 0.8410929785132655 91.36 88.1\n",
      "310 0.6593536123350199 0.8314700999617956 91.47999999999999 88.1\n",
      "320 0.6484318843338339 0.8224334724763818 91.53999999999999 88.1\n",
      "330 0.6380875898377272 0.8139271311498074 91.67999999999999 88.1\n",
      "340 0.6282675069670322 0.8059020877947584 91.8 88.1\n",
      "350 0.6189248317545895 0.7983153006099027 91.86 88.1\n",
      "360 0.610018277623208 0.7911288098884653 91.9 88.2\n",
      "370 0.6015113108409056 0.7843090129813127 92.0 88.4\n",
      "380 0.5933715011863022 0.7778260554534947 92.06 88.5\n",
      "390 0.5855699699907558 0.7716533188895022 92.12 88.4\n",
      "400 0.578080920375462 0.765766988961783 92.30000000000001 88.4\n",
      "410 0.5708812368325391 0.7601456901298671 92.38 88.4\n",
      "420 0.5639501433117526 0.754770175688586 92.47999999999999 88.6\n",
      "430 0.5572689106932343 0.7496230638638269 92.62 88.7\n",
      "440 0.5508206059833523 0.7446886123045066 92.74 88.7\n",
      "450 0.5445898768009942 0.7399525246836046 92.84 88.7\n",
      "460 0.5385627657562181 0.7354017842413831 92.9 88.7\n",
      "470 0.5327265501921842 0.7310245100189227 92.92 88.7\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "tol = 1e-4\n",
    "J_old = 1/tol\n",
    "diff = 1\n",
    "\n",
    "acc_train = []\n",
    "acc_test = []\n",
    "J_val = []\n",
    "steps = []\n",
    "\n",
    "while diff > tol:\n",
    "    J_train, Theta1_grad, Theta2_grad = backprop(Theta1, Theta2, X_train, y_train)\n",
    "\n",
    "    diff = abs(J_old-J_train)\n",
    "    J_old = J_train\n",
    "    J_val.append(J_train)\n",
    "    \n",
    "    step += 1\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        pred_train = predict(Theta1, Theta2, X_train)\n",
    "        pred_test = predict(Theta1, Theta2, X_test)\n",
    "\n",
    "        J_test, T1_grad, T2_grad = backprop(Theta1, Theta2, X_test, y_test)\n",
    "        \n",
    "        acc_train.append(accuracy(pred_train, y_train))\n",
    "        acc_test.append(accuracy(pred_test, y_test))\n",
    "        steps.append(step)\n",
    "        \n",
    "        print(step, J_train, J_test, acc_train[-1], acc_test[-1])\n",
    "\n",
    "    Theta1 -= .5*Theta1_grad\n",
    "    Theta2 -= .5*Theta2_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1, len(J_val)+1), J_val)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Cost function\")\n",
    "plt.gcf().set_size_inches(11, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(steps, acc_train, label='Training dataset')\n",
    "plt.plot(steps, acc_test, label='Testing dataset')\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "plt.gcf().set_size_inches(11, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Theta1.npy', Theta1)\n",
    "np.save('Theta2.npy', Theta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; overflow: hidden;\">\n",
    "     <img src=\"data/D4Sci_logo_full.png\" alt=\"Data For Science, Inc\" align=\"center\" border=\"0\" width=300px> \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
